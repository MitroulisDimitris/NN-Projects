{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17a000f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "\n",
    "from six.moves import xrange\n",
    "\n",
    "import umap\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "training_data = datasets.CIFAR10(root=\"data\", train=True, download=True,\n",
    "                                  transform=transforms.Compose([\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize((0.5,0.5,0.5), (1.0,1.0,1.0))\n",
    "                                  ]))\n",
    "\n",
    "validation_data = datasets.CIFAR10(root=\"data\", train=False, download=True,\n",
    "                                  transform=transforms.Compose([\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize((0.5,0.5,0.5), (1.0,1.0,1.0))\n",
    "                                  ]))\n",
    "\n",
    "data_variance = np.var(training_data.data / 255.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb02ccd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorQuantizer(nn.Module):\n",
    "    def __init(self,num_embeddings,embedding_dim,commitment_cost):\n",
    "        super(VectorQuantizer,self).__init__()\n",
    "    \n",
    "        #codebook dimension and count\n",
    "        self._embedding_dim = embedding_dim\n",
    "        self._num_embeddings = num_embeddings\n",
    "    \n",
    "        self._embedding = nn. Embedding(self._num_embeddings,self._embedding_dim)\n",
    "        self._embedding.weight.data.uniform_(-1/self._num_embeddings)\n",
    "        \n",
    "        #Î² term\n",
    "        self._commitment_cost = commitment_cost\n",
    "       \n",
    "    #we expect a tensor [16,64,32,32] \n",
    "    #then flattened to [16384,64]   \n",
    "    def forward(self):\n",
    "        inputs = inputs.permute(0,2,3,1).contiguous()\n",
    "        input_shape = inputs.shape\n",
    "        \n",
    "        #Flatten input\n",
    "        flat_input = inputs.view(-1,self._embedding_dim)\n",
    "        \n",
    "        #get an array with the discances from each enc.vector \n",
    "        distances = (torch.sum(flat_input**2,dim=1,keepdim=True)#flat_input -> (N,64)\n",
    "                     +torch.sum(self._embedding.weight**2,dim=1)#embedding table -> (K,64)\n",
    "                     -2*torch.matmul(flat_input,self._embedding.weight.t()))\n",
    "\n",
    "        #Encoding\n",
    "        encoding_indices = torch.argmin(distances,dim1=1).unsqueeze(1)\n",
    "        encodings = torch.zeros(encoding_indices.shape[0],self._num_embeddings,device=inputs.device)\n",
    "        encodings.scatter_(1,encoding_indices,1)#differenc representation\n",
    "        \n",
    "        \n",
    "        #Quantize and unflatten\n",
    "        #We mulpitply above representation with codebook\n",
    "        #to end up with quantisized \n",
    "        quantized = torch.matmul(encodings,self._embedding.weight).view(input_shape)\n",
    "        \n",
    "        #Loss\n",
    "        e_latent_loss = F.mse_loss(quantized.detach(),inputs)\n",
    "        q_latent_loss = F.mse_loss(quantized, inputs.detach())\n",
    "        loss = q_latent_loss + self._commitment_cost*e_latent_loss\n",
    "        \n",
    "        quantized = inputs + (quantized - inputs).detach()\n",
    "        avg_probs = torch.mean(encodings, dim=0)\n",
    "        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n",
    "        \n",
    "        # convert quantized from BHWC -> BCHW\n",
    "        return loss, quantized.permute(0, 3, 1, 2).contiguous(), perplexity, encodings  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093dc19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorQuantizerEMA(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, commitment_cost, decay, epsilon=1e-5):\n",
    "        super(VectorQuantizerEMA, self).__init__()\n",
    "        \n",
    "        self._embedding_dim = embedding_dim\n",
    "        self._num_embeddings = num_embeddings\n",
    "        \n",
    "        self._embedding = nn.Embedding(self._num_embeddings, self._embedding_dim)\n",
    "        self._embedding.weight.data.normal_()\n",
    "        self._commitment_cost = commitment_cost\n",
    "        \n",
    "        self.register_buffer('_ema_cluster_size', torch.zeros(num_embeddings))\n",
    "        self._ema_w = nn.Parameter(torch.Tensor(num_embeddings, self._embedding_dim))\n",
    "        self._ema_w.data.normal_()\n",
    "        \n",
    "        self._decay = decay\n",
    "        self._epsilon = epsilon\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # convert inputs from BCHW -> BHWC\n",
    "        inputs = inputs.permute(0, 2, 3, 1).contiguous()\n",
    "        input_shape = inputs.shape\n",
    "        \n",
    "        # Flatten input\n",
    "        flat_input = inputs.view(-1, self._embedding_dim)\n",
    "        \n",
    "        # Calculate distances\n",
    "        distances = (torch.sum(flat_input**2, dim=1, keepdim=True) \n",
    "                    + torch.sum(self._embedding.weight**2, dim=1)\n",
    "                    - 2 * torch.matmul(flat_input, self._embedding.weight.t()))\n",
    "            \n",
    "        # Encoding\n",
    "        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)\n",
    "        encodings = torch.zeros(encoding_indices.shape[0], self._num_embeddings, device=inputs.device)\n",
    "        encodings.scatter_(1, encoding_indices, 1)\n",
    "        \n",
    "        # Quantize and unflatten\n",
    "        quantized = torch.matmul(encodings, self._embedding.weight).view(input_shape)\n",
    "        \n",
    "        # Use EMA to update the embedding vectors\n",
    "        if self.training:\n",
    "            self._ema_cluster_size = self._ema_cluster_size * self._decay + \\\n",
    "                                     (1 - self._decay) * torch.sum(encodings, 0)\n",
    "            \n",
    "            # Laplace smoothing of the cluster size\n",
    "            n = torch.sum(self._ema_cluster_size.data)\n",
    "            self._ema_cluster_size = (\n",
    "                (self._ema_cluster_size + self._epsilon)\n",
    "                / (n + self._num_embeddings * self._epsilon) * n)\n",
    "            \n",
    "            dw = torch.matmul(encodings.t(), flat_input)\n",
    "            self._ema_w = nn.Parameter(self._ema_w * self._decay + (1 - self._decay) * dw)\n",
    "            \n",
    "            self._embedding.weight = nn.Parameter(self._ema_w / self._ema_cluster_size.unsqueeze(1))\n",
    "        \n",
    "        # Loss\n",
    "        e_latent_loss = F.mse_loss(quantized.detach(), inputs)\n",
    "        loss = self._commitment_cost * e_latent_loss\n",
    "        \n",
    "        # Straight Through Estimator\n",
    "        quantized = inputs + (quantized - inputs).detach()\n",
    "        avg_probs = torch.mean(encodings, dim=0)\n",
    "        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n",
    "        \n",
    "        # convert quantized from BHWC -> BCHW\n",
    "        return loss, quantized.permute(0, 3, 1, 2).contiguous(), perplexity, encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5eb2e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(nn.Module):\n",
    "    def __init__(self, in_channels, num_hiddens, num_residual_hiddens):\n",
    "        super(Residual, self).__init__()\n",
    "        self._block = nn.Sequential(\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(in_channels=in_channels,\n",
    "                      out_channels=num_residual_hiddens,\n",
    "                      kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(in_channels=num_residual_hiddens,\n",
    "                      out_channels=num_hiddens,\n",
    "                      kernel_size=1, stride=1, bias=False)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self._block(x)\n",
    "\n",
    "\n",
    "class ResidualStack(nn.Module):\n",
    "    def __init__(self, in_channels, num_hiddens, num_residual_layers, num_residual_hiddens):\n",
    "        super(ResidualStack, self).__init__()\n",
    "        self._num_residual_layers = num_residual_layers\n",
    "        self._layers = nn.ModuleList([Residual(in_channels, num_hiddens, num_residual_hiddens)\n",
    "                             for _ in range(self._num_residual_layers)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(self._num_residual_layers):\n",
    "            x = self._layers[i](x)\n",
    "        return F.relu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0f8651d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channels, num_hiddens, num_residual_layers, num_residual_hiddens):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self._conv_1 = nn.Conv2d(in_channels=in_channels,\n",
    "                                 out_channels=num_hiddens//2,\n",
    "                                 kernel_size=4,\n",
    "                                 stride=2, padding=1)\n",
    "        self._conv_2 = nn.Conv2d(in_channels=num_hiddens//2,\n",
    "                                 out_channels=num_hiddens,\n",
    "                                 kernel_size=4,\n",
    "                                 stride=2, padding=1)\n",
    "        self._conv_3 = nn.Conv2d(in_channels=num_hiddens,\n",
    "                                 out_channels=num_hiddens,\n",
    "                                 kernel_size=3,\n",
    "                                 stride=1, padding=1)\n",
    "        self._residual_stack = ResidualStack(in_channels=num_hiddens,\n",
    "                                             num_hiddens=num_hiddens,\n",
    "                                             num_residual_layers=num_residual_layers,\n",
    "                                             num_residual_hiddens=num_residual_hiddens)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self._conv_1(inputs)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self._conv_2(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self._conv_3(x)\n",
    "        return self._residual_stack(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc2e3c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_channels, num_hiddens, num_residual_layers, num_residual_hiddens):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self._conv_1 = nn.Conv2d(in_channels=in_channels,\n",
    "                                 out_channels=num_hiddens,\n",
    "                                 kernel_size=3, \n",
    "                                 stride=1, padding=1)\n",
    "        \n",
    "        self._residual_stack = ResidualStack(in_channels=num_hiddens,\n",
    "                                             num_hiddens=num_hiddens,\n",
    "                                             num_residual_layers=num_residual_layers,\n",
    "                                             num_residual_hiddens=num_residual_hiddens)\n",
    "        \n",
    "        self._conv_trans_1 = nn.ConvTranspose2d(in_channels=num_hiddens, \n",
    "                                                out_channels=num_hiddens//2,\n",
    "                                                kernel_size=4, \n",
    "                                                stride=2, padding=1)\n",
    "        \n",
    "        self._conv_trans_2 = nn.ConvTranspose2d(in_channels=num_hiddens//2, \n",
    "                                                out_channels=3,\n",
    "                                                kernel_size=4, \n",
    "                                                stride=2, padding=1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self._conv_1(inputs)\n",
    "        \n",
    "        x = self._residual_stack(x)\n",
    "        \n",
    "        x = self._conv_trans_1(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        return self._conv_trans_2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d70663d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "batch_size = 256\n",
    "num_training_updates = 15000\n",
    "\n",
    "num_hiddens = 128\n",
    "num_residual_hiddens = 32\n",
    "num_residual_layers = 2\n",
    "\n",
    "embedding_dim = 64\n",
    "num_embeddings = 512\n",
    "\n",
    "commitment_cost = 0.25\n",
    "\n",
    "decay = 0.99\n",
    "\n",
    "learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72854bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loader = DataLoader(training_data, \n",
    "                             batch_size=batch_size, \n",
    "                             shuffle=True,\n",
    "                             pin_memory=True)\n",
    "\n",
    "validation_loader = DataLoader(validation_data,\n",
    "                               batch_size=32,\n",
    "                               shuffle=True,\n",
    "                               pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "424cda75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, num_hiddens, num_residual_layers, num_residual_hiddens, \n",
    "                 num_embeddings, embedding_dim, commitment_cost, decay=0):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self._encoder = Encoder(3, num_hiddens,\n",
    "                                num_residual_layers, \n",
    "                                num_residual_hiddens)\n",
    "        self._pre_vq_conv = nn.Conv2d(in_channels=num_hiddens, \n",
    "                                      out_channels=embedding_dim,\n",
    "                                      kernel_size=1, \n",
    "                                      stride=1)\n",
    "        if decay > 0.0:\n",
    "            self._vq_vae = VectorQuantizerEMA(num_embeddings, embedding_dim, \n",
    "                                              commitment_cost, decay)\n",
    "        else:\n",
    "            self._vq_vae = VectorQuantizer(num_embeddings, embedding_dim,\n",
    "                                           commitment_cost)\n",
    "        self._decoder = Decoder(embedding_dim,\n",
    "                                num_hiddens, \n",
    "                                num_residual_layers, \n",
    "                                num_residual_hiddens)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self._encoder(x)\n",
    "        z = self._pre_vq_conv(z)\n",
    "        loss, quantized, perplexity, _ = self._vq_vae(z)\n",
    "        x_recon = self._decoder(quantized)\n",
    "\n",
    "        return loss, x_recon, perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe14300",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "0747f93ff6db21b2db2bf35ad4858dd0825b9c21797c41b4cc32097944ab3f10"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
