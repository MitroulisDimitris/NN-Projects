{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bda6f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\pl_bolts\\models\\self_supervised\\amdim\\amdim_module.py:35: UnderReviewWarning: The feature generate_power_seq is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  \"lr_options\": generate_power_seq(LEARNING_RATE_CIFAR, 11),\n",
      "D:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\pl_bolts\\models\\self_supervised\\amdim\\amdim_module.py:93: UnderReviewWarning: The feature FeatureMapContrastiveTask is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  contrastive_task: Union[FeatureMapContrastiveTask] = FeatureMapContrastiveTask(\"01, 02, 11\"),\n",
      "D:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\pl_bolts\\losses\\self_supervised_learning.py:234: UnderReviewWarning: The feature AmdimNCELoss is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  self.nce_loss = AmdimNCELoss(tclip)\n",
      "D:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\pl_bolts\\datamodules\\experience_source.py:18: UnderReviewWarning: The feature warn_missing_pkg is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  warn_missing_pkg(\"gym\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import datasets\n",
    "import pytorch_lightning as pl\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from pl_bolts.models.autoencoders.components import (\n",
    "    resnet18_decoder,\n",
    "    resnet18_encoder,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "tensor_transform = transforms.ToTensor()\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device:',device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a0f6381",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root=\"~/torch_datasets\", train=True, transform=transform, download=True\n",
    ")\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root=\"~/torch_datasets\", train=False, transform=transform, download=True\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=256, shuffle=True, num_workers=0, pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=32, shuffle=False, num_workers=0\n",
    ")\n",
    "\n",
    "example_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=1, shuffle=True, num_workers=0,drop_last=True,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa1439af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper functions\n",
    "def plot():\n",
    "    f, axarr = plt.subplots(2)\n",
    "\n",
    "    for i, item in enumerate(image):\n",
    "    # Reshape the array for plotting\n",
    "        item = item.reshape(-1, 28, 28)\n",
    "        axarr[0].imshow(item[0].cpu())\n",
    "\n",
    "    for i, item in enumerate(reconstructed):\n",
    "        item = item.reshape(-1, 28, 28).cpu()\n",
    "        item = item.detach().numpy()\n",
    "        axarr[1].imshow(item[0])\n",
    "        \n",
    "        \n",
    "        \n",
    "def showExample():\n",
    "    for image, _ in example_loader:\n",
    "        f, axarr = plt.subplots(2)\n",
    "        image = image.reshape(-1,28*28).to(device)\n",
    "\n",
    "        model.to(device)\n",
    "        recon = model(image)\n",
    "\n",
    "        image = image.reshape(-1, 28, 28)\n",
    "        axarr[0].imshow(image[0].cpu())\n",
    "\n",
    "\n",
    "        recon = recon.reshape(-1, 28, 28).to('cpu')\n",
    "        axarr[1].imshow(recon[0].detach().numpy())\n",
    "\n",
    "        break\n",
    "\n",
    "def add_noise(inputs,variance):\n",
    "    noise = torch.randn_like(inputs)\n",
    "    return inputs + variance*noise\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b7f315",
   "metadata": {},
   "source": [
    "### KL Divergence intuition\n",
    "How we are minimizing KL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dab7d44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence(self,z,mu,std):\n",
    "    p = torch.distributions.Normal(torch.zeros_like(mu), torch.ones_like(std))\n",
    "    q = torch.distributions.Normal(mu, std)\n",
    "    \n",
    "    log_pz = p.log_prob(z)\n",
    "    log_qzx = q.log_prob(z)\n",
    "    \n",
    "    kl = (log_qxz - log_pz)\n",
    "    \n",
    "    #sum over last dim to go from single dim distr. to multi-dim\n",
    "    kl = kl.sum(-1)\n",
    "    return kl\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1283e874",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 28*28  #784\n",
    "hidden_size = 128\n",
    "code_size = 32\n",
    "\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        #Encoder \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size,hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size,code_size)\n",
    "        )\n",
    "        \n",
    "        #Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(code_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, input_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.model = nn.Sequential(\n",
    "        \n",
    "        )    \n",
    "        \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d91435f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(pl.LightningModule):\n",
    "    def __init__(self, enc_out_dim=512, latent_dim=256, input_height=32):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.encoder = resnet18_encoder(False,False)\n",
    "        self.decoder = resnet18_decoder(latent_dim = latent_dim,\n",
    "                                        input_height = input_height,\n",
    "                                        first_conv=False,\n",
    "                                        maxpool1=False\n",
    "                                        \n",
    "                                        )\n",
    "        \n",
    "        self.fc_mu = nn.Linear(enc_out_dim,latent_dim)\n",
    "        self.fc_var = nn.Linear(enc_out_dim,latent_dim)\n",
    "        \n",
    "        #for gaussian distribution\n",
    "        self.log_scale = nn.Parameter(torch.Tensor([0.0]))\n",
    "        \n",
    "        def configure_optimizers(self):\n",
    "            return torch.optim.Adam(self.parameters(),lr=1e-4)\n",
    "        \n",
    "        \n",
    "        def gaussian_likelihood(self,x_hat,logscale,x):\n",
    "            scale = torch.exp(logscale)\n",
    "            mean = x_hat\n",
    "            dist = torch.distributions.Normal(mean,scale)\n",
    "            \n",
    "            \n",
    "            #prob of seing z under p(x|z)\n",
    "            log_pxz = dist.log_prob(x)\n",
    "            return log_pxz.sun(dim=(1,2,3))\n",
    "        \n",
    "        def kl_divergence(self,z,mu,std):\n",
    "            #define probabilities\n",
    "            p = torch.distributions.Normal(torch.zeros_like(mu),\n",
    "                                           torch.ones_like(std))\n",
    "            \n",
    "            #2. get probabilbities from equation\n",
    "            log_qzx = q.log_prob(z)\n",
    "            log_pz = p.log_prob(z)\n",
    "            \n",
    "            \n",
    "            #kl\n",
    "            kl = (log_qzx- log_pz)\n",
    "            kl = kl.sum(-1)\n",
    "            return kl\n",
    "        \n",
    "        \n",
    "        \n",
    "        def\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87c0657",
   "metadata": {},
   "source": [
    "First distribution:  \n",
    "**q(z|x)**  \n",
    "We generate mu, std from the encoder\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "69e678ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DIMITRIS\\AppData\\Local\\Temp\\ipykernel_15220\\3126387324.py:10: UnderReviewWarning: The feature resnet18_encoder is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  self.encoder = resnet18_encoder(False,False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Shape torch.Size([1, 3, 32, 32])\n",
      "Encoded Image Shape torch.Size([1, 512])\n",
      "μ torch.Size([1, 256])\n",
      "log_var torch.Size([1, 256])\n",
      "z shape: torch.Size([1, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DIMITRIS\\AppData\\Local\\Temp\\ipykernel_15220\\3126387324.py:11: UnderReviewWarning: The feature resnet18_decoder is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  self.decoder = resnet18_decoder(latent_dim = latent_dim,\n"
     ]
    }
   ],
   "source": [
    "vae  = VAE()\n",
    "\n",
    "#image from cifar-10(3 channels, 32*32)\n",
    "x = torch.rand(1,3,32,32)\n",
    "print('Image Shape {}'.format(x.shape))\n",
    "\n",
    "#Encode x to get mu and variance parameters\n",
    "x_encoded = vae.encoder(x)\n",
    "mu, log_var = vae.fc_mu(x_encoded),vae.fc_var(x_encoded)\n",
    "\n",
    "print('Encoded Image Shape {}'.format(x_encoded.shape))\n",
    "print('μ {}'.format(mu.shape))\n",
    "print('log_var {}'.format(log_var.shape))\n",
    "\n",
    "# SAMPLE Z from Q(Z|x)\n",
    "std = torch.exp(log_var / 2)\n",
    "q = torch.distributions.Normal(mu, std)\n",
    "z = q.rsample()\n",
    "\n",
    "print('z shape:', z.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33aa886",
   "metadata": {},
   "source": [
    "The second distribution:  \n",
    "**p(z)**   \n",
    "is the prior which we will fix to specific location(0,1)  \n",
    "By doing this, KL Willl force q(z|x) to move close to p\n",
    "\n",
    "At start, distributions will look like this:\n",
    "![](markdown/distributions.jpg)\n",
    "\n",
    "\n",
    "\n",
    "And over time, the q will mmove close to p, as it has learnable parameters\n",
    "\n",
    "![](markdown/distributions-over-time.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "511a3c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero = torch.zeros_like(mu)\n",
    "one = torch.ones_like(std)\n",
    "p = torch.distributions.Normal(zero,one)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c11be7f",
   "metadata": {},
   "source": [
    "The third distribution:  \n",
    "**p(x|z)**  \n",
    "(or reconstruction) will be used to measure the probability of seeing the image give that z was sampled\n",
    "\n",
    "Note: x_hat is **not** an image. They are parameters for a distribution  \n",
    "When used with MNIST dataset, they can form an image, but for colored images this isn't true "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "55898553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "x_hat = vae.decoder(z)\n",
    "print(x_hat.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3249cc",
   "metadata": {},
   "source": [
    "Lastly, we use x to parameterize a likehood distribution so that we can measure the probability of an input uder the (very highly dimensional) distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b8f0ec35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 32, 32])\n",
      "torch.Size([1])\n",
      "recon Loss -4153.076171875\n"
     ]
    }
   ],
   "source": [
    "log_scale = nn.Parameter(torch.Tensor([0.0]))\n",
    "scale = torch.exp(log_scale)\n",
    "dist = torch.distributions.Normal(x_hat,scale)\n",
    "log_pxz = dist.log_prob(x)\n",
    "print(log_pxz.shape)\n",
    "\n",
    "\n",
    "#sum across all channels, pixels\n",
    "log_pxz = log_pxz.sum(dim=(1,2,3))\n",
    "print(log_pxz.shape)\n",
    "print('recon Loss {}'.format(log_pxz.item()))\n",
    "\n",
    "recon_loss = log_pxz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41849cfc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
