{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4a28888",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import stuff\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import random\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "edecc0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import files\n",
    "with open('datasets/allan_poe.txt', 'r', encoding='utf-8') as file:\n",
    "    alan = file.read()\n",
    "\n",
    "with open('datasets/robert_frost.txt', 'r', encoding='utf-8') as file:\n",
    "    rob = file.read()\n",
    "\n",
    "alanPoe = 'datasets/allan_poe.txt'\n",
    "robert = 'datasets/robert_frost.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b7b88acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper functions\n",
    "def tokenize_txt(txt):\n",
    "    uniqueWords = {}\n",
    "    word2vec = []\n",
    "    vec2word = []\n",
    "    idx=0  \n",
    "    for index, word in enumerate(txt.split(\" \")):\n",
    "        if word not in uniqueWords:\n",
    "            uniqueWords[word] = idx\n",
    "            idx += 1\n",
    "        word2vec.append(uniqueWords.get(word))\n",
    "        vec2word.append(word)\n",
    "        \n",
    "    return uniqueWords, word2vec, vec2word\n",
    "    \n",
    "    \n",
    "    \n",
    "def UnknownIndex(df,table):\n",
    "    uniqueWords = []\n",
    "    # for each row\n",
    "    for index, row in df.iterrows():\n",
    "        row = re.sub(\" , [0-2][0-9]:[0-5][0-9]\", \"\",str(row.values))\n",
    "        row = re.sub(\"[-,|!|.|?|\\\"}\\][\\']\", \"\", row)\n",
    "        words = [w.lower() for w in row.split()]\n",
    "\n",
    "        for word in words:\n",
    "            if word not in uniqueWords and word not in table:\n",
    "                uniqueWords.append(word)\n",
    "                \n",
    "    \n",
    "    return uniqueWords\n",
    "\n",
    "def cleanup(item):\n",
    "    item = re.sub(\" , [0-2][0-9]:[0-5][0-9]\", \"\",item)\n",
    "    item = re.sub(\"[-()-,|!|.|?|\\\"{}\\][\\']\", \"\", item)\n",
    "    return item\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1854e0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LO! Death ha\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def normalize(txt):\n",
    "    clean_txt = []\n",
    "    #txt = re.sub(\"\\\\n\", \" \",txt)\n",
    "    txt = re.sub(\"\\\\'\", \" \",txt)\n",
    "    txt = txt.replace('\\u2009', '')\n",
    "    for index, word in enumerate(txt.split(\"\\\\n\")):\n",
    "        if word:\n",
    "            word = re.sub(\" , [0-2][0-9]:[0-5][0-9]\", \"\",word)\n",
    "            print(word)\n",
    "            return None\n",
    "            word = word.translate(str.maketrans('', '', string.punctuation))\n",
    "            word = re.sub(\" {1,}\", \"\", word)\n",
    "            word = word.lower()\n",
    "            clean_txt.append(word)\n",
    "    return clean_txt\n",
    "\n",
    "\n",
    "\n",
    "def normalize_rows(txt):\n",
    "    for line in open(txt, encoding=\"utf8\"):\n",
    "        print(line)\n",
    "        break\n",
    "\n",
    "alan2 = normalize_rows(alanPoe)\n",
    "#print(alan2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1185789f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4351\n"
     ]
    }
   ],
   "source": [
    "# correct lenght 1250\n",
    "\n",
    "Aunique, Aword2vector, Avector2word = tokenize_txt(alan)\n",
    "Runique, Rword2vector, Rvector2word = tokenize_txt(rob)\n",
    "\n",
    "input_text = Avector2word + Rvector2word\n",
    "\n",
    "labels = [0] * len(Avector2word)\n",
    "print(len(labels))\n",
    "labels += [1] * len(Rvector2word)\n",
    "\n",
    "\n",
    "unique = Aunique.copy()\n",
    "unique.update(Runique)\n",
    "\n",
    "vector2word = Avector2word+Rvector2word\n",
    "word2vector = Aword2vector+Rword2vector\n",
    "\n",
    "train_text, test_text, Ytrain, Ytest = train_test_split(vector2word, labels)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf873684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "851\n",
      "586\n",
      "think\n"
     ]
    }
   ],
   "source": [
    "# should print the same\n",
    "print(unique.get('most')) \n",
    "\n",
    "\n",
    "print(word2vector[1249])\n",
    "print(vector2word[1249])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db490a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# markov model\n",
    "def count_sequence(array, target):\n",
    "    count = 0\n",
    "    targ_len = len(target)\n",
    "    \n",
    "    for i in range(len(array) - targ_len+1):\n",
    "        if array[i:i+targ_len] == target:\n",
    "            count+=1\n",
    "    \n",
    "    return count\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def markov_model(sequence, order=2,eps=1):\n",
    "    #tokenize sequence\n",
    "    unique,word2vec,vec2word = tokenize_txt(sequence)\n",
    "    \n",
    "    lenght = len(vec2word)\n",
    "    A = np.empty((lenght, lenght))\n",
    "    \n",
    "    markov_model = {}  \n",
    "    \n",
    "    #loop from start to end-order\n",
    "    for i, word in enumerate(vec2word[:-order]):\n",
    "        con = []\n",
    "        \n",
    "        # add next words to context\n",
    "        context = vec2word[i:i+order+1]\n",
    "\n",
    "        next_char = vec2word[i+order]\n",
    "    \n",
    "        \n",
    "    \n",
    "    return markov_model\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "def predict_seq(markov_model, initial_context, length):\n",
    "    current_context = initial_context\n",
    "    generated_text = current_context\n",
    "    for _ in range(length):\n",
    "        if current_context not in markov_model:\n",
    "            break\n",
    "        next_chars = list(markov_model[current_context].keys())\n",
    "        next_char = random.choice(next_chars)\n",
    "        generated_text += next_char\n",
    "        current_context = generated_text[-len(initial_context):]\n",
    "    return generated_text\n",
    "\n",
    "    unique = tokenize_txt(sequence)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return null\n",
    "    \n",
    "    #start from start+count\n",
    "    for i, word in enumerate(sequence.split(\" \")[count:]):\n",
    "        break\n",
    "        context = []    \n",
    "        \n",
    "        \n",
    "        for index in range(count):\n",
    "            context.append(index+i)\n",
    "        \n",
    "        for j, word2 in enumerate(sequence[count:]):\n",
    "            #num =  \n",
    "            #denom = c\n",
    "            #A[i][j] = \n",
    "            pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "3e6fac99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize A\n",
    "V = len(unique)\n",
    "\n",
    "A0 = np.ones((V,V))\n",
    "pi0 = np.ones(V)\n",
    "\n",
    "\n",
    "A1 = np.ones((V,V))\n",
    "pi1 = np.ones(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94236e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_counts(text_as_int, A,pi):\n",
    "    for tokens in text_as_int:\n",
    "        last_idx = None\n",
    "        for idx in tokens:\n",
    "            if last_idx is None:\n",
    "                # it's the first word in a sentence\n",
    "                pi[idx]+=1\n",
    "            else:\n",
    "                # the last word exists, so count a transition\n",
    "                A[last_idx, idx] +=1\n",
    "            #update last idex\n",
    "            last_idx = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3bcd064f",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_text_int' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4436\\2538347386.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# why thiss??? Why merge and then split again?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcompute_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_text_int\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYtrain\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mA0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpi0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mcompute_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_text_int\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYtrain\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mA1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpi1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_text_int' is not defined"
     ]
    }
   ],
   "source": [
    "# why thiss??? Why merge and then split again? \n",
    "compute_counts([t for t, y in zip(train_text_int, Ytrain) if y==0], A0,pi0)\n",
    "compute_counts([t for t, y in zip(train_text_int, Ytrain) if y==1], A1,pi1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "adc26b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: transform text to arrays by line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b04c828e",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'A0' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4436\\2491208169.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mA0\u001b[0m \u001b[1;33m/=\u001b[0m \u001b[0mA0\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mpi0\u001b[0m \u001b[1;33m/=\u001b[0m\u001b[0mpi0\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mA1\u001b[0m \u001b[1;33m/=\u001b[0m \u001b[0mA1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mpi1\u001b[0m \u001b[1;33m/=\u001b[0m\u001b[0mpi1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'A0' is not defined"
     ]
    }
   ],
   "source": [
    "A0 /= A0.sum(axis=1, keepdims=True)\n",
    "pi0 /=pi0.sum()\n",
    "\n",
    "A1 /= A1.sum(axis=1, keepdims=True)\n",
    "pi1 /=pi1.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047ef93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "logA0 = np.log(A0)\n",
    "logpi0 = np.log(pi0)\n",
    "\n",
    "logA1 = np.log(A1)\n",
    "logpi1 = np.log(pi1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
