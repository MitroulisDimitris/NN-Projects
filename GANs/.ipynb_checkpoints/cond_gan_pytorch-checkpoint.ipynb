{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a493c415",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Tue Jan 17 13:24:23 2023\n",
    "\n",
    "@author: DIMITRIS\n",
    "\n",
    "Description: Conditional GAN Network, for eaducational purposed\n",
    "Status: Working, needs some fine-tuning\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# %% Import and stuff\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import torchvision.utils as vutils\n",
    "from  torch.utils import data\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "from ignite.metrics import FID\n",
    "\n",
    "from ignite.engine import *\n",
    "from ignite.handlers import *\n",
    "from ignite.metrics import *\n",
    "from ignite.utils import *\n",
    "from ignite.contrib.metrics.regression import *\n",
    "from ignite.contrib.metrics import *\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "\n",
    "NUM_EPOCHS = 150\n",
    "LR = 0.0002\n",
    "LATENT_DIM = 100\n",
    "IMG_SIZE = 28\n",
    "CHANNELS = 1\n",
    "B1 = 0.5\n",
    "B2 = 0.999\n",
    "\n",
    "\n",
    "GEN_STATE_DICT = \"gen_state_dict\"\n",
    "DISC_STATE_DICT = \"disc_state_dict\"\n",
    "GEN_OPTIMIZER = \"gen_optimizer\"\n",
    "DISC_OPTIMIZER = \"disc_optimizer\"\n",
    "G_LOSSES = \"g_losses\"\n",
    "D_LOSSES = \"d_losses\"\n",
    "\n",
    "\n",
    "\n",
    "SHUFFLE = True\n",
    "PIN_MEMORY = True\n",
    "NUM_WORKERS = 0\n",
    "BATCH_SIZE = 2000\n",
    "\n",
    "specific_latent = torch.tensor([[0.7628, 0.1779, 0.3978, 0.3606, 0.6387,\n",
    "         0.3044, 0.8340, 0.3884, 0.9313, 0.5635, 0.1994, 0.6934, 0.5326,\n",
    "         0.3676, 0.5342, 0.9480, 0.4120, 0.5845, 0.4035, 0.5298, 0.0177,\n",
    "         0.5605, 0.6453, 0.9576, 0.7153, 0.1923, 0.8122, 0.0937, 0.5744,\n",
    "         0.5951, 0.8890, 0.4838, 0.5707, 0.6760, 0.3738, 0.2796, 0.1549,\n",
    "         0.8220, 0.2800, 0.4051, 0.2553, 0.1831, 0.0046, 0.9021, 0.0264,\n",
    "         0.2327, 0.8261, 0.0534, 0.1582, 0.4087, 0.9047, 0.1409, 0.6864,\n",
    "         0.1439, 0.3432, 0.1072, 0.5907, 0.6756, 0.6942, 0.6814, 0.3368,\n",
    "         0.4138, 0.8030, 0.7024, 0.3309, 0.7288, 0.2193, 0.1954, 0.9948,\n",
    "         0.1201, 0.9483, 0.7407, 0.4849, 0.6500, 0.8649, 0.7405, 0.4725,\n",
    "         0.5373, 0.6541, 0.5444, 0.7425, 0.8940, 0.3580, 0.3905, 0.8924,\n",
    "         0.2995, 0.3726, 0.5399, 0.3057, 0.3380, 0.8313, 0.1137, 0.0120,\n",
    "         0.7714, 0.2561, 0.2569, 0.2994, 0.7648, 0.2413, 0.6101\n",
    "        ]])\n",
    "\n",
    "\n",
    "img_shape = (CHANNELS, IMG_SIZE, IMG_SIZE)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device:{}'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f4e4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% helper funcitons\n",
    "\n",
    "\n",
    "def save_checkpoint(state, filename):\n",
    "    print(\"=> Saving chekpoint\")\n",
    "    torch.save(state, filename)\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint):\n",
    "    generator.load_state_dict(checkpoint[GEN_STATE_DICT])\n",
    "    optimizer_G.load_state_dict(checkpoint[GEN_OPTIMIZER])\n",
    "    discriminator.load_state_dict(checkpoint[DISC_STATE_DICT])\n",
    "    optimizer_D.load_state_dict(checkpoint[DISC_OPTIMIZER])\n",
    "    G_losses = checkpoint[G_LOSSES]\n",
    "    D_losses = checkpoint[D_LOSSES]\n",
    "    \n",
    "\n",
    "\n",
    "# takes input tensor and return a tensor of same size but every element has different value\n",
    "def build_fake_labels(old_list):\n",
    "  \n",
    "    new_list = []\n",
    "\n",
    "    for i, x in enumerate(old_list):\n",
    "\n",
    "        if (i % 10) != x:\n",
    "            new_list.append(i % 10)\n",
    "        else:\n",
    "            new_list.append((x.item()+1) % 10)\n",
    "\n",
    "    return torch.tensor(new_list, dtype=torch.int64).to(device)\n",
    "\n",
    "\n",
    "def add_noise(inputs, variance):\n",
    "    noise = torch.randn_like(inputs)\n",
    "    return inputs + variance*noise\n",
    "\n",
    "def gen_image(caption=-1,randomLatent=True):\n",
    "    generator.to('cpu')\n",
    "    discriminator.to('cpu')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image,_ in train_loader:\n",
    "            f, axarr = plt.subplots(1)\n",
    "            \n",
    "            if randomLatent:\n",
    "                latent = torch.rand_like(torch.Tensor(1,100))\n",
    "            else:\n",
    "                latent = specific_latent\n",
    "                \n",
    "            if caption == -1:\n",
    "                caption = random.randint(0, 9)\n",
    "            \n",
    "            caption = torch.tensor(caption, dtype=torch.int64)\n",
    "            fake_image = generator(latent,caption)  \n",
    "           \n",
    "            \n",
    "            #axarr.imshow(add_noise(image[0][0],0.5))    \n",
    "            axarr.imshow(fake_image[0][0])   \n",
    "            print(\"Supposed to be %d\" %caption.item())\n",
    "    \n",
    "            break\n",
    "        \n",
    "def discriminate_image(caption=-1,genOrReal=0):#random.randint(0, 1)):\n",
    "    generator.to('cpu')\n",
    "    discriminator.to('cpu')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for  i, (imgs, labels) in enumerate(example_loader):\n",
    "            f, axarr = plt.subplots(1)\n",
    "            \n",
    "            fake_labels = build_fake_labels(labels.to(device))\n",
    "            labels = labels.to('cpu')\n",
    "            z = Variable(Tensor(np.random.normal(0, 1, (1,LATENT_DIM)))).cpu()\n",
    "            if caption == -1:\n",
    "                caption = random.randint(0, 9)\n",
    "            caption = torch.tensor(caption, dtype=torch.int64)\n",
    "            \n",
    "            \n",
    "            #feed discriminator fake image, expect \"0\" output\n",
    "            if genOrReal == 0:\n",
    "                fake_image = generator(z,caption)\n",
    "                axarr.imshow(fake_image[0].reshape(-1, 28, 28)[0])\n",
    "                pred = discriminator(fake_image,caption).detach()\n",
    "                print(\"Discriminator Prediction: {},Should be: {}, label = {}\".format(pred,\"0\",caption))\n",
    "            #feed discriminator real image, expect \"1\" output\n",
    "            else:\n",
    "                fake_image = generator(z,labels[0])\n",
    "                axarr.imshow(imgs[0].reshape(-1, 28, 28)[0])\n",
    "                pred = discriminator(imgs.detach(),labels[0].detach()).detach()\n",
    "                print(\"Discriminator Prediction: {},Should be: {}, label= {}\".format(pred,\"1\",labels[0]+1))\n",
    "            \n",
    "    \n",
    "            break        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e59f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%train data\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "    ])\n",
    "\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root=\"~/torch_datasets\", train=True, transform=transform, download=True\n",
    ")\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root=\"~/torch_datasets\", train=False, transform=transform, download=True\n",
    ")\n",
    "\n",
    "train_loader = data.DataLoader(\n",
    "                                train_dataset,\n",
    "                                batch_size=BATCH_SIZE,\n",
    "                                shuffle=False,\n",
    "                                num_workers=NUM_WORKERS,\n",
    "                                pin_memory=False\n",
    "                                )\n",
    "\n",
    "test_loader = data.DataLoader(\n",
    "                                test_dataset,\n",
    "                                batch_size=32,\n",
    "                                shuffle=True,\n",
    "                                num_workers=0\n",
    "                                )\n",
    "\n",
    "example_loader = data.DataLoader(\n",
    "                                train_dataset,\n",
    "                                batch_size=1,\n",
    "                                shuffle=True,\n",
    "                                num_workers=0,\n",
    "                                drop_last=True,\n",
    "                                )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97624d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Detective: fake or no fake -> 1 output [0, 1]\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(2, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 1)\n",
    "\n",
    "        self.emb = nn.Embedding(10, 50)\n",
    "        self.emb_fc = nn.Linear(50, 784)\n",
    "\n",
    "        self.nconv1 = nn.Conv2d(2, 64, kernel_size=5)\n",
    "        self.nconv2 = nn.Conv2d(64, 128, kernel_size=5)\n",
    "        self.pool = nn.AvgPool2d(kernel_size=3)\n",
    "        self.pool2 = nn.AvgPool2d(kernel_size=2)\n",
    "        self.nfc1 = nn.Linear(1152, 164)\n",
    "        self.nfc2 = nn.Linear(164, 1)\n",
    "\n",
    "    # oldWay flag to select between 2 train methods, not sure which is best yet\n",
    "    def forward(self, x, c, oldWay=False):\n",
    "\n",
    "        c = self.emb(c)\n",
    "        c = self.emb_fc(c)\n",
    "        c = c.view(-1, 1, 28, 28)\n",
    "        x = torch.cat((c, x), 1)  # concat image[1,28,28] with text [1,28,28]\n",
    "\n",
    "        x = F.leaky_relu(self.nconv1(x))\n",
    "        x = F.leaky_relu(self.nconv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.pool2(x)\n",
    "        x = x.view(-1, 1152)\n",
    "        x = F.leaky_relu(self.nfc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.nfc2(x)\n",
    "\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# %% Generate Fake Data: output like real data [1, 28, 28] and values -1, 1\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(LATENT_DIM, 7*7*63)  # [n,100]->[n,3087]\n",
    "        self.ct1 = nn.ConvTranspose2d(64, 32, 4, stride=2)  # [n, 64, 16, 16] [32,..,..]\n",
    "        self.ct2 = nn.ConvTranspose2d(32, 16, 4, stride=2)  # [n, 32, , ]->[n, 16, 34, 34]\n",
    "        self.conv = nn.Conv2d(16, 1, kernel_size=7)  # [n, 16, 34, 34]-> [n, 1, 28, 28]\n",
    "        \n",
    "        self.emb = nn.Embedding(10, 50) \n",
    "        self.label_lin = nn.Linear(50, 49)\n",
    "        self.conv_x_c = nn.ConvTranspose2d(65, 64, 4, stride=2)  # upsample [65,7,7] -> [64,14,14]\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        # Pass latent space input into linear layer and reshape\n",
    "        x = self.lin1(x)  # (n,100) -> (n,3187)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = x.view(-1, 63, 7, 7)  # (n,3187) -> (63,7,7)\n",
    "        \n",
    "        #Encode label\n",
    "        c = self.emb(c)  # (n,) -> (n,50)\n",
    "        c = self.label_lin(c)  # (n,50) -> (n,49)\n",
    "        c = c.view(-1, 1, 7, 7)  # (n,49) -> (n,1,7,7)\n",
    "        x = torch.cat((c, x), 1) # concat image[63,7,7] with text [1,7,7]\n",
    "\n",
    "        x = self.ct1(x)  # [n, 64, 16, 16] [32,34,34]\n",
    "        x = F.leaky_relu(x)\n",
    "\n",
    "        # Upsample to 34x34 (16 feature maps)\n",
    "        x = self.ct2(x)\n",
    "        x = F.leaky_relu(x)\n",
    "\n",
    "        # Convolution to 28x28 (1 feature map)\n",
    "        x = self.tanh(self.conv(x))\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc07c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Loss fucntion, optimizers\n",
    "loss_func = nn.BCELoss()\n",
    "d_loss_func = nn.BCELoss()\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "generator = Generator().to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    generator.cuda()\n",
    "    discriminator.cuda()\n",
    "    loss_func.cuda()\n",
    "\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=LR,betas=(B1 ,B2))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=LR,betas=(B1 ,B2))\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e61a176",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_checkpoint(torch.load(\"cond_gan_pytorch11.pth.tar\",map_location=(device)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b608e244",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fake_images = []\n",
    "real_images = []\n",
    "with torch.no_grad():\n",
    "    generator.to('cpu')\n",
    "    discriminator.to('cpu')\n",
    "    \n",
    "    for i, (imgs,_) in enumerate(train_loader):\n",
    "        #real_images = imgs\n",
    "        \n",
    "        caption = random.randint(0, 9)   \n",
    "        caption = torch.tensor(caption, dtype=torch.int64)\n",
    "        latent = torch.rand_like(torch.Tensor(1,100))\n",
    "        fake_images = generator(latent,caption)\n",
    "        \n",
    "        \n",
    "        for i in range(len(imgs[:,0,0,0])):\n",
    "            \n",
    "            caption = random.randint(0, 9)   \n",
    "            caption = torch.tensor(caption, dtype=torch.int64)\n",
    "            latent = torch.rand_like(torch.Tensor(1,100))\n",
    "            \n",
    "\n",
    "            \n",
    "            fake_image = generator(latent,caption)\n",
    "            fake_images = torch.cat((fake_images, fake_image), 0)\n",
    "\n",
    "        break\n",
    "    \n",
    "    fake_images = fake_images[:2000,:,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48014174",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import inception_v3\n",
    "import torch\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor, Normalize\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import inception_v3\n",
    "from scipy import linalg\n",
    "import numpy as np\n",
    "\n",
    "fake_images = torch.cat([fake_images, fake_images, fake_images], dim=1)\n",
    "fake_images = transforms.Compose([\n",
    "    transforms.Resize((299, 299)),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "])(fake_images)\n",
    "\n",
    "\n",
    "\n",
    "# load a pre-trained Inception-v3 model\n",
    "inception_model = inception_v3(pretrained=True, aux_logits=True,)\n",
    "inception_model.to(device)\n",
    "inception_model.eval()\n",
    "\n",
    "# compute the feature representations of the real and fake images\n",
    "real_features = []\n",
    "fake_features = []\n",
    "for batch in train_loader:\n",
    "    images, _ = batch\n",
    "    images = images.to(device)\n",
    "    with torch.no_grad():\n",
    "        images = torch.cat([images,images,images],dim=1)\n",
    "        images = transforms.Compose([\n",
    "            transforms.Resize((299, 299)),\n",
    "            transforms.Grayscale(num_output_channels=3),\n",
    "        ])(images)\n",
    "        \n",
    "        features = inception_model(images)[0].view(images.size(0), -1)\n",
    "    real_features.append(features.cpu().numpy())\n",
    "with torch.no_grad():\n",
    "    features = inception_model(fake_images)[0].view(fake_images.size(0), -1)\n",
    "fake_features.append(features.cpu().numpy())\n",
    "\n",
    "# calculate the mean and covariance of the feature representations\n",
    "real_features = np.concatenate(real_features, axis=0)\n",
    "fake_features = np.concatenate(fake_features, axis=0)\n",
    "mu1, sigma1 = np.mean(real_features, axis=0), np.cov(real_features, rowvar=False)\n",
    "mu2, sigma2 = np.mean(fake_features, axis=0), np.cov(fake_features, rowvar=False)\n",
    "\n",
    "# calculate the FID score\n",
    "mu_diff = mu1 - mu2\n",
    "sigma_diff_sqrt = linalg.sqrtm(sigma1 @ sigma2)\n",
    "fid_score = np.real(np.trace(sigma1 + sigma2 - 2*sigma_diff_sqrt)) + np.dot(mu_diff, mu_diff)\n",
    "print(f'FID score: {fid_score:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645ba33b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
